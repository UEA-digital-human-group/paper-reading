
## Relighting Images in the Wild With a Self-Supervised Siamese Auto-Encoder

### WACV 2021

---

Yang Liu, Alexandros Neophytou, Sunando Sengupta, Eric Sommerlade

University of Surrey and Microsoft, Reading.

---

## Resources

[paper](https://openaccess.thecvf.com/content/WACV2021/html/Liu_Relighting_Images_in_the_Wild_With_a_Self-Supervised_Siamese_Auto-Encoder_WACV_2021_paper.html)


---

## Research Goal

A self-supervised method for relighting of single view images in the wild. 

![goal](assets/goal.png)

---

#### Prior Work  

- Human faces (masked):
  - Estimating surface normals based on an SFSNet network (an end-to-end learning framework for producing an accurate decomposition of an unconstrained human face image into shape, reflectance and illuminance.)
  - Jointly estimating facial features and lighting features
- Environment images:
  - Most of the previous works need 2D/3D geometric priors or RGB-D sensors
- Photo style transfer:
  - GANs widely used (pix2pix, CycleGAN)

---

## Key Idea

Train an auto-encoder which decomposes the image into two embeddings: one for illumination and one for content

Motivation: A labelled dataset for supervised learning techniques is not hard to attain.

---

#### Challenge 1

How can we separate illumination from content?

---

#### Challenge 1: Step 1

Augmenting images such that geometry remains the same but lighting direction changes.

- Flip images
- Rotate images
- Invert images

---

#### Challenge 1: Step 2

Pair up augmented image with original image and train a Siamese auto-encoder network.

The assumption is that each pair of images have the same content but different illumination.

---

#### Challenge 2

How can we generate semantically meaningful light representation?

---

#### Challenge 2: Step 1

Design a spherical harmonic loss to force the illumination embedding to take the form of Laplace's spherical harmonics.

In this way, the lighting can be meaningfully controlled.

---

## Method

---

#### Relighting Auto-encoding


![autoencoder](assets/autoencoder.png)

If target and source illumination embeddings are the
same, the network becomes a reconstruction network.
Otherwise, it is a relighting network.

---

Three objective functions are used to train the autoencoder:

- reconstruction loss
- Spherical Harmonic loss 
- discriminator loss

---

Reconstruction loss

Since there is no ground truth illumination embedding, a siamese network is used to compute the reconstruction loss.

![siamese](assets/siamese.png)

The auto-encoders are identical and share weights. 

---

Reconstruction loss

![siamese](assets/siamese.png)

Mean absolute error (MAE) is computed for the reconstructed imagese and lighting embeddings. MAE also computed over image gradients for preserve edges.

![recloss](assets/recloss.png)

---

Spherical Harmonic loss

Reconstruction loss alone can't guarantee that the network can relight images. 



---

## Probably Symmetric Objects

Assume that depth and albedo are symmetric about a fixed vertical plane.

Then, they require that:

$$d \approx \text{flip} (d')$$

and

$$a \approx \text{flip} (a')$$

---

In practice, these constraints are used in training by computing a second reconstructed image:

$$\hat{\mathbf{I}}' = \Pi(\Lambda(a',d',l),d',w)$$


---

The model is trained to encourage both 
$\mathbf{I} \approx \hat{\mathbf{I}}$ and 
$\mathbf{I} \approx \hat{\mathbf{I}}'$

---

## Loss Function

![overview](assets/loss.png)

The same loss is calculated for the symmetric reconstruction, $\mathcal{L}(\hat{\mathbf{I}}', \mathbf{I}, \sigma')$.

---

## Loss Function

![overview](assets/fullloss.png)


---

## Image Formation

Nothing unusual.

- Perspective camera with assumed narrow field of view
- Viewpoint represents a rotation and translation along $x,y,z$
- Normals are computed, multiplied by light direction and added to ambient illumination.
- Above is multiplied by albedo to get illuminated texture.
- Light is represented as a spherical sector


---

#### Implementation

- Depth and albedo are generated by (separate) encoder-decoder networks
  - encoder-decoders do not use skip connections because input and output images are not spatially aligned (since the output is in the canonical viewpoint)
- Viewpoint and lighting are regressed using simple encoder networks
- Trained using Adam, 50k iterations

---

![overview](assets/overview.png)

---

# Experiments

---

#### Data

- Human face datasets
  - CelebA, 3DFAW and BFM
- Cats
  - Combine two datasets, one with 10k examples and another with 1.2k
- Cars
  - Synthetic cars rendered from ShapeNet

---

#### Results

![results](assets/results.png)<!-- .element height="50%" width="50%" -->

SIDE: Scale-invariant depth error

MAD: Mean angle deviation (normals)

---

#### Qualitative Results

![results](assets/qualitative1.png)<!-- .element height="70%" width="70%" -->



---

#### Qualitative Results

![results](assets/qualitative2.png)<!-- .element height="70%" width="70%" -->

---

#### Limitations

![results](assets/limitations.png)<!-- .element height="70%" width="70%" -->

- Fails under extreme lighting
  - assumes Lambertian shading model (ignores shadow and specularity)
- Fails with dark, noisy textures
- Fails with extreme angles

---

# Questions

